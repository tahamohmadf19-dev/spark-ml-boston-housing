# Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… - Spark ML Boston Housing

## ğŸ“‹ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª

1. [Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©](#Ø§Ù„Ù…Ù„ÙØ§Øª-Ø§Ù„Ù…ØªØ§Ø­Ø©)
2. [ÙƒÙŠÙÙŠØ© ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯](#ÙƒÙŠÙÙŠØ©-ØªØ´ØºÙŠÙ„-Ø§Ù„ÙƒÙˆØ¯)
3. [Ø´Ø±Ø­ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†](#Ø´Ø±Ø­-Ø§Ù„ÙƒÙˆØ¯-Ø§Ù„Ù…Ø­Ø³Ù‘Ù†)
4. [Ø§Ù„ØªØ®ØµÙŠØµ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ](#Ø§Ù„ØªØ®ØµÙŠØµ-Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª-Ø§Ù„Ø®Ø§ØµØ©-Ø¨Ùƒ)

---

## Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

### 1. `improved_spark_ml_boston.py` â­
**Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† ÙƒÙ€ Python script**
- ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† terminal
- Ù…Ù†Ø¸Ù… ÙÙŠ Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª

### 2. `Improved_Spark_ML_Boston_Housing.ipynb`
**Ø§Ù„Ù†ÙˆØªØ¨ÙˆÙƒ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†**
- Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Jupyter Notebook
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø´Ø±ÙˆØ­Ø§Øª ÙˆØªÙˆØ«ÙŠÙ‚
- Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„Ø¹Ø±Ø¶

### 3. `original_notebook.ipynb`
**Ø§Ù„Ù†ÙˆØªØ¨ÙˆÙƒ Ø§Ù„Ø£ØµÙ„ÙŠ**
- Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©

---

## ÙƒÙŠÙÙŠØ© ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: ØªØ´ØºÙŠÙ„ Python Script

```bash
# 1. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
pip install -r requirements.txt

# 2. ØªØ­Ø¶ÙŠØ± Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# Ø¶Ø¹ Ù…Ù„Ù "Boston House Price Data.csv" ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯

# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯
python improved_spark_ml_boston.py
```

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: ØªØ´ØºÙŠÙ„ Jupyter Notebook

```bash
# 1. ØªØ´ØºÙŠÙ„ Jupyter
jupyter notebook

# 2. ÙØªØ­ Ø§Ù„Ù†ÙˆØªØ¨ÙˆÙƒ
# Ø§ÙØªØ­ Ù…Ù„Ù: Improved_Spark_ML_Boston_Housing.ipynb

# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ cells ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰
# Ø£Ùˆ Run All Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Cell
```

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Colab

```python
# 1. Ø§Ø±ÙØ¹ Ø§Ù„Ù†ÙˆØªØ¨ÙˆÙƒ Ø¥Ù„Ù‰ Google Drive
# 2. Ø§ÙØªØ­Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Colab
# 3. Ø«Ø¨Ù‘Øª PySpark ÙÙŠ Ø£ÙˆÙ„ cell:

!pip install pyspark

# 4. Ø§Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
from google.colab import files
uploaded = files.upload()

# 5. Ø´ØºÙ‘Ù„ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù€ cells
```

---

## Ø´Ø±Ø­ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†

### Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©

```python
# 1. INITIALIZATION - ØªÙ‡ÙŠØ¦Ø© Spark
spark = initialize_spark()

# 2. DATA LOADING - ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df = load_data(spark, "Boston House Price Data.csv")

# 3. EXPLORATION - Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
check_missing_values(df)
visualize_target(df)

# 4. FEATURE ENGINEERING - Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª
df_fe = feature_engineering(df)

# 5. DATA SPLITTING - ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
train_df, val_df, test_df = split_data(df_fe)

# 6. OUTLIER HANDLING - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
bounds = calculate_outlier_bounds(train_df, numeric_cols)
train_cap = cap_outliers(train_df, bounds)

# 7. STANDARDIZATION - Ø§Ù„ØªØ·Ø¨ÙŠØ¹
stats_row = calculate_standardization_stats(train_cap, numeric_cols)
train_final = standardize(train_cap, numeric_cols, stats_row)

# 8. MODEL TRAINING - ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
lr_model = train_linear_regression(...)
dt_model = train_decision_tree(...)
rf_model = train_random_forest_with_cv(...)  # Ù…Ø¹ CrossValidator

# 9. EVALUATION - Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
results = evaluate_model(...)
compare_models([lr_results, dt_results, rf_results])

# 10. FEATURE IMPORTANCE - Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
importance_df = analyze_feature_importance(best_rf, feature_cols)

# 11. TEST EVALUATION - Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
test_pred = evaluate_on_test(rf_model, test_final)

# 12. MODEL PERSISTENCE - Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
save_model(rf_model)
```

---

## Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§

### 1. ØªÙ‡ÙŠØ¦Ø© Spark

```python
spark = initialize_spark()
```

**Ø§Ù„ÙˆØ¸ÙŠÙØ©:** Ø¥Ù†Ø´Ø§Ø¡ Spark Session  
**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:** ÙƒØ§Ø¦Ù† SparkSession

---

### 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

```python
df = load_data(spark, file_path="path/to/data.csv")
```

**Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:**
- `spark`: SparkSession object
- `file_path`: Ù…Ø³Ø§Ø± Ù…Ù„Ù CSV

**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:** Spark DataFrame

---

### 3. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª

```python
df_fe = feature_engineering(df)
```

**Ø§Ù„ÙˆØ¸ÙŠÙØ©:** Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ©:
- `RM_LSTAT = RM Ã— LSTAT`
- `NOX_INDUS = NOX Ã— INDUS`
- `DIS_RAD = DIS Ã— RAD`

**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:** DataFrame Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©

---

### 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©

```python
# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¯ÙˆØ¯
bounds = calculate_outlier_bounds(train_df, numeric_cols)

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
train_cap = cap_outliers(train_df, bounds)
val_cap = cap_outliers(val_df, bounds)
test_cap = cap_outliers(test_df, bounds)
```

**Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©:** IQR Method
- Lower: Q1 - 1.5 Ã— IQR
- Upper: Q3 + 1.5 Ã— IQR

**Ù…Ù‡Ù…:** Ø§Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ù† train ÙÙ‚Ø· Ù„Ù…Ù†Ø¹ data leakage!

---

### 5. Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Standardization)

```python
# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
stats_row = calculate_standardization_stats(train_cap, numeric_cols)

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
train_final = standardize(train_cap, numeric_cols, stats_row)
val_final = standardize(val_cap, numeric_cols, stats_row)
test_final = standardize(test_cap, numeric_cols, stats_row)
```

**Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©:** `(x - mean) / std`

**Ù…Ù‡Ù…:** Ø§Ø³ØªØ®Ø¯Ù… mean Ùˆ std Ù…Ù† train ÙÙ‚Ø·!

---

### 6. ØªØ¯Ø±ÙŠØ¨ Random Forest Ù…Ø¹ CrossValidator

```python
rf_model, best_rf = train_random_forest_with_cv(
    train_final, 
    assembler, 
    target_column
)
```

**Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ÙØ®ØªØ¨Ø±Ø©:**
- `numTrees`: [50, 100, 200]
- `maxDepth`: [5, 8, 10]
- `minInstancesPerNode`: [1, 2, 4]

**Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹:** 27 combination

**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:**
- `rf_model`: Ø£ÙØ¶Ù„ Pipeline model
- `best_rf`: Ø£ÙØ¶Ù„ RandomForestRegressor

---

### 7. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

```python
# ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯
results = evaluate_model(train_pred, val_pred, "Model Name", target_column)

# Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
results_df = compare_models([lr_results, dt_results, rf_results])
```

**Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³:**
- **RMSE**: Root Mean Squared Error
- **MAE**: Mean Absolute Error
- **RÂ²**: Coefficient of Determination

---

### 8. ØªØ­Ù„ÙŠÙ„ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª

```python
importance_df = analyze_feature_importance(best_rf, feature_cols)
```

**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:**
- Pandas DataFrame Ù…Ø¹ Feature Ùˆ Importance
- Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø£Ù‡Ù…ÙŠØ©
- Ø·Ø¨Ø§Ø¹Ø© Ø£Ù‡Ù… 3 Ù…ÙŠØ²Ø§Øª

---

### 9. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

```python
save_model(rf_model, path="./my_model")
```

**Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø­Ù‚Ø§Ù‹:**

```python
from pyspark.ml import PipelineModel
loaded_model = PipelineModel.load("./my_model")
```

---

## Ø§Ù„ØªØ®ØµÙŠØµ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ

### 1. ØªØºÙŠÙŠØ± Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

ÙÙŠ Ø¯Ø§Ù„Ø© `main()`:

```python
# Ù‚Ø¨Ù„
file_path = "Boston House Price Data.csv"

# Ø¨Ø¹Ø¯
file_path = "/path/to/your/data.csv"
```

---

### 2. ØªØºÙŠÙŠØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù

```python
# Ù‚Ø¨Ù„
target_column = "PRICE"

# Ø¨Ø¹Ø¯
target_column = "YourTargetColumn"
```

---

### 3. ØªØ®ØµÙŠØµ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª

ÙÙŠ Ø¯Ø§Ù„Ø© `feature_engineering()`:

```python
def feature_engineering(df):
    df_fe = (
        df
        # Ø£Ø¶Ù Ù…ÙŠØ²Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ© Ù‡Ù†Ø§
        .withColumn("Feature1_Feature2", col("Feature1") * col("Feature2"))
        .withColumn("Feature3_squared", col("Feature3") ** 2)
    )
    
    # Ø­Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø­Ø°ÙÙ‡Ø§
    cols_to_drop = ["OldFeature1", "OldFeature2"]
    df_fe = df_fe.drop(*cols_to_drop)
    
    return df_fe
```

---

### 4. ØªØ®ØµÙŠØµ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Random Forest

ÙÙŠ Ø¯Ø§Ù„Ø© `train_random_forest_with_cv()`:

```python
# Ù‚Ø¨Ù„
paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [50, 100, 200]) \
    .addGrid(rf.maxDepth, [5, 8, 10]) \
    .addGrid(rf.minInstancesPerNode, [1, 2, 4]) \
    .build()

# Ø¨Ø¹Ø¯ - Ø¬Ø±Ù‘Ø¨ Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ©
paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [100, 200, 300]) \
    .addGrid(rf.maxDepth, [8, 10, 15]) \
    .addGrid(rf.minInstancesPerNode, [2, 5, 10]) \
    .addGrid(rf.maxBins, [32, 64]) \  # Ù…Ø¹Ø§Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠ
    .build()
```

---

### 5. ØªØºÙŠÙŠØ± Ù†Ø³Ø¨ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…

ÙÙŠ Ø¯Ø§Ù„Ø© `split_data()`:

```python
# Ù‚Ø¨Ù„
train_df, val_df, test_df = split_data(df, 0.7, 0.15, 0.15)

# Ø¨Ø¹Ø¯ - Ù…Ø«Ù„Ø§Ù‹ 80/10/10
train_df, val_df, test_df = split_data(df, 0.8, 0.1, 0.1)
```

---

### 6. Ø¥Ø¶Ø§ÙØ© Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯

```python
def train_gradient_boosted_tree(train_final, assembler, target_column):
    """Train Gradient Boosted Tree Regressor"""
    from pyspark.ml.regression import GBTRegressor
    
    gbt = GBTRegressor(featuresCol="features", labelCol=target_column, seed=42)
    gbt_pipeline = Pipeline(stages=[assembler, gbt])
    
    print("\nTraining Gradient Boosted Tree...")
    gbt_model = gbt_pipeline.fit(train_final)
    print("GBT training completed!")
    
    return gbt_model

# ÙÙŠ main()
gbt_model = train_gradient_boosted_tree(train_final, assembler, target_column)
gbt_train_pred = gbt_model.transform(train_final)
gbt_val_pred = gbt_model.transform(val_final)
gbt_results = evaluate_model(gbt_train_pred, gbt_val_pred, "GBT", target_column)

# Ø£Ø¶Ù Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
results_df = compare_models([lr_results, dt_results, rf_results, gbt_results])
```

---

## Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

### 1. ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø¹Ù„Ù‰ Spark Cluster

```python
spark = SparkSession.builder \
    .appName("BostonHousingRegression") \
    .master("spark://master:7077") \  # Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù€ cluster
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()
```

---

### 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Parquet

```python
df = spark.read.parquet("path/to/data.parquet")
```

---

### 3. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

```python
# Ø­ÙØ¸ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
test_pred.select("PRICE", "prediction").write.csv("predictions.csv", header=True)

# Ø­ÙØ¸ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
results_df.to_csv("model_comparison.csv", index=False)

# Ø­ÙØ¸ Feature Importance
importance_df.to_csv("feature_importance.csv", index=False)
```

---

## Ù†ØµØ§Ø¦Ø­ Ù…Ù‡Ù…Ø©

### âœ… Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª

1. **Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ø³ØªØ®Ø¯Ù… seed Ù„Ù„Ù€ reproducibility**
   ```python
   randomSplit([0.7, 0.15, 0.15], seed=42)
   ```

2. **Ø§Ø­Ø³Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ù† train ÙÙ‚Ø·**
   ```python
   # âœ… ØµØ­
   bounds = calculate_outlier_bounds(train_df, numeric_cols)
   
   # âŒ Ø®Ø·Ø£
   bounds = calculate_outlier_bounds(df, numeric_cols)  # data leakage!
   ```

3. **Ø§Ø³ØªØ®Ø¯Ù… cache() Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨ÙƒØ«Ø±Ø©**
   ```python
   train_final.cache()
   val_final.cache()
   ```

4. **Ø§Ø³ØªØ®Ø¯Ù… Pipeline Ø¯Ø§Ø¦Ù…Ø§Ù‹**
   ```python
   pipeline = Pipeline(stages=[assembler, model])
   ```

---

### âš ï¸ Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø©

1. **Ù†Ø³ÙŠØ§Ù† ØªØ­Ø¯ÙŠØ« Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**
   - ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ø¯ÙŠØ« `file_path` ÙÙŠ `main()`

2. **Ø¹Ø¯Ù… Ù…Ø·Ø§Ø¨Ù‚Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©**
   - ØªØ£ÙƒØ¯ Ø£Ù† `target_column` Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

3. **ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¨Ø¯ÙˆÙ† cluster**
   - Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Spark cluster

4. **Ù†Ø³ÙŠØ§Ù† stop() Ù„Ù„Ù€ Spark session**
   - Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø£Ù†Ù‡Ù Ø¨Ù€ `spark.stop()`

---

## Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©

### Ø³: ÙƒÙŠÙ Ø£ØºÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„Ù€ folds ÙÙŠ CrossValidatorØŸ

```python
crossval = CrossValidator(
    estimator=rf_pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=5,  # ØºÙŠÙ‘Ø± Ù‡Ù†Ø§ (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ 3)
    seed=42
)
```

---

### Ø³: ÙƒÙŠÙ Ø£Ø³ØªØ®Ø¯Ù… metric Ù…Ø®ØªÙ„Ù Ù„Ù„ØªÙ‚ÙŠÙŠÙ…ØŸ

```python
# Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ù€ MAE Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† RMSE
evaluator = RegressionEvaluator(
    labelCol=target_column,
    predictionCol="prediction",
    metricName="mae"  # Ø£Ùˆ "r2"
)
```

---

### Ø³: ÙƒÙŠÙ Ø£Ø­ÙØ¸ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©ØŸ

```python
# Ø¨Ø¹Ø¯ plt.show()
plt.savefig("comparison_plot.png", dpi=300, bbox_inches='tight')
```

---

### Ø³: Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø·ÙŠØ¡ Ø¬Ø¯Ø§Ù‹ØŒ ÙƒÙŠÙ Ø£Ø³Ø±Ù‘Ø¹Ù‡ØŸ

1. Ø§Ø³ØªØ®Ø¯Ù… `cache()` Ø£ÙƒØ«Ø±
2. Ù‚Ù„Ù„ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ hyperparameters ÙÙŠ ParamGrid
3. Ù‚Ù„Ù„ `numFolds` ÙÙŠ CrossValidator
4. Ø§Ø³ØªØ®Ø¯Ù… Spark cluster

---

## Ø§Ù„Ø®Ù„Ø§ØµØ©

Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµÙ‡ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ù„Ø£ÙŠ dataset regression. 

**Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:**
- Ø±Ø§Ø¬Ø¹ [README.md](README.md) Ù„Ù„Ù†Ø¸Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
- Ø±Ø§Ø¬Ø¹ [improvements_summary.md](improvements_summary.md) Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
- Ø±Ø§Ø¬Ø¹ [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) Ù„Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„

**Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:** https://github.com/tahamohmadf19-dev/spark-ml-boston-housing

---

**Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚! ğŸš€**
